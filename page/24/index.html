<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/panda-180-3.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/panda-32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/panda-16.png">
  <link rel="mask-icon" href="/images/panda.svg" color="#222">
  <meta name="baidu-site-verification" content="m1ei8mEvXD">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"mxxct4git.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="君子不器">
<meta property="og:type" content="website">
<meta property="og:title" content="猫熊小才天の书院">
<meta property="og:url" content="https://mxxct4git.github.io/page/24/index.html">
<meta property="og:site_name" content="猫熊小才天の书院">
<meta property="og:description" content="君子不器">
<meta property="og:locale">
<meta property="article:author" content="Mxxct">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://mxxct4git.github.io/page/24/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-Hans'
  };
</script>

  <title>猫熊小才天の书院</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">猫熊小才天の书院</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://mxxct4git.github.io/2018/11/16/Hive-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/panda-180.png">
      <meta itemprop="name" content="Mxxct">
      <meta itemprop="description" content="君子不器">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="猫熊小才天の书院">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/16/Hive-2/" class="post-title-link" itemprop="url">Hive sql函数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-11-16 19:03:08" itemprop="dateCreated datePublished" datetime="2018-11-16T19:03:08+08:00">2018-11-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/Hive/" itemprop="url" rel="index"><span itemprop="name">Hive</span></a>
                </span>
            </span>

          
            <span id="/2018/11/16/Hive-2/" class="post-meta-item leancloud_visitors" data-flag-title="Hive sql函数" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2018/11/16/Hive-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/11/16/Hive-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Hive-sql函数"><a href="#Hive-sql函数" class="headerlink" title="Hive sql函数"></a>Hive sql函数</h2><h3 id="一、关系运算"><a href="#一、关系运算" class="headerlink" title="一、关系运算"></a>一、关系运算</h3><ol>
<li><p>等值比较: &#x3D;<br> <code>select 1 from dual where 1 = 2;</code></p>
</li>
<li><p>等值比较:&lt;&#x3D;&gt;<br>a &lt;&#x3D;&gt; b</p>
</li>
<li><p>不等值比较: &lt;&gt;和!&#x3D;<br>a !&#x3D; b || a &lt;&gt; b</p>
</li>
<li><p>小于比较: &lt;<br>a &lt; b</p>
</li>
<li><p>小于等于比较: &lt;&#x3D;<br>a &lt;&#x3D; b</p>
</li>
<li><p>大于比较: &gt;<br>a &gt; b</p>
</li>
<li><p>大于等于比较: &gt;&#x3D;<br>a &gt;&#x3D; b</p>
</li>
<li><p>区间比较？？？？？  </p>
<pre><code> # step1 设定区间分类个数，此处设置为10
 m=10
 
 # step2 求解字段 result 的最大值和最小值
 section=`hive -e &quot;
 select max(result) as max_num,
    min(result) as min_num
 from  tmp
 &quot;`
 max_num=`echo -e &quot;$&#123;section&#125;&quot; | cut -f1`
 min_num=`echo -e &quot;$&#123;section&#125;&quot; | cut -f2`
 
 # step3 求解区间的长度
 len_section=`hive -e &quot;
 select ($&#123;max_num&#125; - $&#123;min_num&#125;) / $&#123;m&#125;
 from   dual
 &quot;`
 
 # step4 统计每个区间的个数
 hive -e &quot;
 select $&#123;min_num&#125; + floor((result - $&#123;min_num&#125;) / $&#123;len_section&#125;) * $&#123;len_section&#125; as section_flag,
    count(*) as num  
 from   tmp
 &quot;
</code></pre>
</li>
<li><p>空值判断: IS NULL<br> <code>select 1 from dual where A is null;</code></p>
</li>
<li><p>非空判断: IS NOT NULL<br><code>select 1 from dual where A is not null;</code></p>
</li>
<li><p>LIKE比较: A LIKE B<br><strong>B中字符”_“表示任意单个字符，而字符”%”表示任意数量的字符</strong><br><strong>否定比较的时候用 NOT A LIKE B</strong></p>
<pre><code>select 1 from dual where ‘key&#39; like &#39;foot%&#39;;  
select 1 from dual where ‘key &#39; like &#39;foot____&#39;;
</code></pre>
</li>
<li><p>JAVA的LIKE操作: A RLIKE B<br>如果字符串A或者字符串B为NULL，则返回NULL；如果字符串A符合JAVA正则表达式B的正则语法，则为TRUE；否则为FALSE  </p>
<pre><code>select 1 from dual where &#39;123456&#39; rlike &#39;^\\d+$&#39;    判断一个字符串是否都是数字
</code></pre>
</li>
<li><p>REGEXP操作: A REGEXP B<br>功能和RLIKE一样  </p>
<pre><code>select 1 from dual where ‘key&#39; REGEXP &#39;^f.*r$&#39;;
</code></pre>
</li>
</ol>
<h3 id="二、数学运算"><a href="#二、数学运算" class="headerlink" title="二、数学运算"></a>二、数学运算</h3><ol>
<li><p>加法操作: +</p>
</li>
<li><p>减法操作: –<br> <strong>结果的数值类型等于A的类型和B的类型的最小父类型</strong><br> 比如int ± int 一般结果为int类型，而int ± double 一般结果为double类型  </p>
</li>
<li><p>乘法操作: *<br> <strong>结果的数值类型等于A的类型和B的类型的最小父类型</strong><br> 如果A乘以B的结果超过默认结果类型的数值范围，则需要通过cast将结果转换成范围更大的数值类型</p>
</li>
<li><p>除法操作: &#x2F;？？？？？<br> 返回A除以B的结果。结果的数值类型为double  </p>
<pre><code> 注意：hive 中最高精度的数据类型是 double, 只精确到小数点后16位，做除法运算时要特别注意  
 hive&gt;select ceil(28.0/6.999999999999999999999) from dual limit 1;  
 结果为4  
 hive&gt;select ceil(28.0/6.99999999999999) from dual limit 1;  
 结果为5
</code></pre>
</li>
<li><p>取余操作: %<br> 返回A除以B的余数。结果的数值类型等于A的类型和B的类型的最小父类型  </p>
<pre><code> 注意：精度在 hive 中是个很大的问题，类似这样的操作最好通过 round 指定精度  
 hive&gt; select round(8.4 % 4 , 2) from dual;  
 0.4  
</code></pre>
</li>
<li><p>位与操作: &amp;<br> 返回A和B按位进行与操作的结果。结果的数值类型等于A的类型和B的类型的最小父类型  </p>
<pre><code> hive&gt; select 4 &amp; 8 from dual;  
 0  
 hive&gt; select 6 &amp; 4 from dual;  
 4
</code></pre>
</li>
<li><p>位或操作: |<br> 返回A和B按位进行或操作的结果。结果的数值类型等于A的类型和B的类型的最小父类型  </p>
<pre><code> hive&gt; select 4 | 8 from dual;  
 12  
 hive&gt; select 6 | 8 from dual;  
 14
</code></pre>
</li>
<li><p>位异或操作: ^<br> 返回A和B按位进行异或操作的结果。结果的数值类型等于A的类型和B的类型的最小父类型</p>
<pre><code> hive&gt; select 4 ^ 8 from dual;  
 12  
 hive&gt; select 6 ^ 4 from dual;  
 2
</code></pre>
</li>
<li><p>位取反操作: ~<br>返回A按位取反操作的结果。结果的数值类型等于A的类型。</p>
<pre><code> hive&gt; select ~6 from dual;  
 -7  
 hive&gt; select ~4 from dual;  
 -5
</code></pre>
</li>
</ol>
<blockquote>
<p><strong>所有正整数的按位取反是其本身+1的负数</strong><br><strong>所有负整数的按位取反是其本身+1的绝对值</strong><br><strong>零的按位取反是 -1</strong></p>
</blockquote>
<h3 id="三、逻辑运算"><a href="#三、逻辑运算" class="headerlink" title="三、逻辑运算"></a>三、逻辑运算</h3><ol>
<li><p>逻辑与操作: AND 、&amp;&amp;</p>
</li>
<li><p>逻辑或操作: OR 、||</p>
</li>
<li><p>逻辑非操作: NOT、!<br> NOT A：如果A为FALSE，或者A为NULL，则为TRUE；否则为FALSE  </p>
<pre><code> select 1 from dual where  not 1=2 ;
</code></pre>
</li>
</ol>
<h3 id="四、复合类型构造函数"><a href="#四、复合类型构造函数" class="headerlink" title="四、复合类型构造函数"></a>四、复合类型构造函数</h3><ol>
<li><p>map结构<br> map (key1, value1, key2, value2,…)<br> 根据输入的key和value对构建map类型</p>
<pre><code> hive&gt; Create table lxw_test as select map(&#39;100&#39;,&#39;tom&#39;,&#39;200&#39;,&#39;mary&#39;)as t from lxw_dual;
 hive&gt; describe lxw_test;
 t   map&lt;string,string&gt;
 hive&gt; select t from lxw_test;
 &#123;&quot;100&quot;:&quot;tom&quot;,&quot;200&quot;:&quot;mary&quot;&#125;
 
 hive&gt; create table employee(id string, perf map&lt;string, int&gt;)   
 &gt; ROW FORMAT DELIMITED  
 &gt; FIELDS TERMINATED BY &#39;\t&#39;
 &gt; COLLECTION ITEMS TERMINATED BY &#39;,&#39;   
 &gt; MAP KEYS TERMINATED BY &#39;:&#39;;
 1   job:80,team:60,person:70  
 2   job:60,team:80  
 3   job:90,team:70,person:100
 hive&gt; load data local inpath &#39;/home/oracle/emp.txt&#39; into table employee;
 hive&gt; select perf[&#39;person&#39;] from employee;
 OK  
 70  
 NULL  
 100
 hive&gt; select perf[&#39;person&#39;] from employee where perf[&#39;person&#39;] is not null;
 70  
 100
</code></pre>
</li>
</ol>
<blockquote>
<p>‘MAP KEYS TERMINATED BY’ ：key value分隔符</p>
</blockquote>
<ol start="2">
<li><p>struct结构<br> struct(val1, val2, val3,…)<br> 根据输入的参数构建结构体struct类型</p>
<pre><code> hive&gt; create table student_test(id INT, info struct&lt;name:STRING, age:INT&gt;)  
 &gt; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; 
 &gt; COLLECTION ITEMS TERMINATED BY &#39;:&#39;;
 hive&gt;load data local inpath &#39;/home/oracle/student_test.txt&#39; into table student_test;
 1,zhou:30  
 2,yan:30  
 3,chen:20  
 4,li:80
 hive&gt; select info.age from student_test;  
 30  
 30  
 20  
 80
</code></pre>
</li>
</ol>
<blockquote>
<p>‘FIELDS TERMINATED BY’ ：字段与字段之间的分隔符<br>‘COLLECTION ITEMS TERMINATED BY’ ：一个字段各个item的分隔符</p>
</blockquote>
<ol start="3">
<li><p>named_struct结构<br> named_struct(name1,val1,name2,val2,name3,val3,…)<br> 使用给定的表达式，构造一个指定列名的 struct 数据结构</p>
<pre><code> hive&gt; select named_struct(&#39;a&#39;,1,&#39;b&#39;,&#39;aaa&#39;,&#39;c&#39;,FALSE) from lxw1234; 
 &#123;&quot;a&quot;:1,&quot;b&quot;:&quot;aaa&quot;,&quot;c&quot;:false&#125;
</code></pre>
</li>
<li><p>array结构<br> array(val1, val2,…)<br> 根据输入的参数构建数组array类型</p>
<pre><code> hive&gt; create table lxw_test as selectarray(&quot;tom&quot;,&quot;mary&quot;,&quot;tim&quot;) as t from lxw_dual;
 hive&gt; describe lxw_test;
 t   array&lt;string&gt;
 hive&gt; select t from lxw_test;
 [&quot;tom&quot;,&quot;mary&quot;,&quot;tim&quot;]
 
 hive&gt; create table class_test(name string, student_id_list array&lt;INT&gt;)
 034,1:2:3:4  
 035,5:6  
 036,7:8:9:10
 hive&gt; select student_id_list[3] from class_test;
 4  
 NULL  
 10
</code></pre>
</li>
<li><p>create_union (tag, val1, val2, …)<br> 使用给定的 tag 和表达式，构造一个 uniontype 数据结构。tag 表示使用第 tag 个 表达式作为 uniontype 的 value</p>
<pre><code> hive&gt; select create_union(0,&#39;ss&#39;,array(1,2,3)) from lxw1234; 
 &#123;0:&quot;ss&quot;&#125;
 hive&gt; select create_union(1,&#39;ss&#39;,array(1,2,3)) from lxw1234; 
 &#123;1:[1,2,3]&#125;
</code></pre>
</li>
</ol>
<h3 id="五、复合类型操作符"><a href="#五、复合类型操作符" class="headerlink" title="五、复合类型操作符"></a>五、复合类型操作符</h3><ol>
<li><p>获取array中的元素 A[n]<br> 返回数组A中的第n个变量值。数组的起始下标为0。比如，A是个值为[‘foo’, ‘bar’]的数组类型，那么A[0]将返回’foo’,而A[1]将返回’bar’  </p>
<pre><code> hive&gt; create table lxw_test as selectarray(&quot;tom&quot;,&quot;mary&quot;,&quot;tim&quot;) as t from lxw_dual;
 hive&gt; select t[0],t[1],t[2] from lxw_test;
 tom   mary   tim

 hive&gt; select array(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;)[1] from lxw1234;  
 b
</code></pre>
</li>
<li><p>获取map中的元素 M[key]<br> 返回map类型M中，key值为指定值的value值。比如，M是值为{‘f’ -&gt; ‘foo’, ‘b’-&gt; ‘bar’, ‘all’ -&gt; ‘foobar’}的map类型，那么M[‘all’]将会返回’foobar’</p>
<pre><code> hive&gt; Create table lxw_test as selectmap(&#39;100&#39;,&#39;tom&#39;,&#39;200&#39;,&#39;mary&#39;) as t from lxw_dual;
 hive&gt; select t[&#39;200&#39;],t[&#39;100&#39;] from lxw_test;
 mary   tom

 hive&gt; select map(&#39;k1&#39;,&#39;v1&#39;)[&#39;k1&#39;];  
 v1
</code></pre>
</li>
<li><p>获取struct中的元素 S.x<br> 返回结构体S中的x字段。比如，对于结构体struct foobar {int foo, int bar}，foobar.foo返回结构体中的foo字段</p>
<pre><code> hive&gt; create table lxw_test as select struct(&#39;tom&#39;,&#39;mary&#39;,&#39;tim&#39;)as t from lxw_dual;
 hive&gt; describe lxw_test;
 t   struct&lt;col1:string,col2:string,col3:string&gt;
 hive&gt; select t.col1,t.col3 from lxw_test;
 tom tim

 hive&gt; select named_struct(&#39;a&#39;,1,&#39;b&#39;,&#39;aaa&#39;,&#39;c&#39;,FALSE).c;  
 false
</code></pre>
</li>
</ol>
<h3 id="六、数值计算函数"><a href="#六、数值计算函数" class="headerlink" title="六、数值计算函数"></a>六、数值计算函数</h3><ol>
<li><p>取整函数: round(double a)<br> 返回double类型的整数值部分 （遵循四舍五入）</p>
<pre><code> hive&gt; select round(3.1415926) from dual;  
 3  
 hive&gt; select round(3.5) from dual;  
 4
</code></pre>
</li>
<li><p>指定精度取整函数: round(double a, int d)<br> 返回指定精度d的double类型</p>
<pre><code> hive&gt; select round(3.1415926,4) from dual;  
 3.1416
</code></pre>
</li>
<li><p>向下取整函数: floor(double a)<br> 返回等于或者小于该double变量的最大的整数</p>
<pre><code> hive&gt; select floor(3.1415926) from dual;  
 3  
 hive&gt; select floor(25) from dual;  
 25
</code></pre>
</li>
<li><p>向上取整函数: ceil(double a)<br> 返回等于或者大于该double变量的最小的整数</p>
<pre><code> hive&gt; select ceil(3.1415926) from dual;  
 4  
 hive&gt; select ceil(46) from dual;  
 46
</code></pre>
</li>
<li><p>向上取整函数: ceiling(double a)<br>与ceil功能相同</p>
<pre><code> hive&gt; select ceiling(3.1415926) from dual;  
 4  
 hive&gt; select ceiling(46) from dual;  
 46
</code></pre>
</li>
<li><p>取随机数函数: rand(),rand(int seed)<br> 返回一个0到1范围内的随机数。如果指定种子seed，则会等到一个稳定的随机数序列</p>
<pre><code> hive&gt; select rand() from dual;  
 0.5577432776034763
</code></pre>
</li>
<li><p>自然指数函数: exp(double a)<br> 返回自然对数e的a次方</p>
<pre><code> hive&gt; select exp(2) from dual;  
 7.38905609893065
</code></pre>
</li>
<li><p>以10为底对数函数: log10(double a)<br> 返回以10为底的a的对数</p>
<pre><code> hive&gt; select log10(100) from dual;  
 2.0
</code></pre>
</li>
<li><p>以2为底对数函数: log2(double a)<br> 返回以2为底的a的对数</p>
<pre><code> hive&gt; select log2(8) from dual;  
 3.0
</code></pre>
</li>
<li><p>对数函数: log(double base, double a)<br>返回以base为底的a的对数</p>
<pre><code>hive&gt; select log(4,256) from dual;
4.0
</code></pre>
</li>
<li><p>幂运算函数: pow(double a, double p)<br>返回a的p次幂</p>
<pre><code>hive&gt; select pow(2,4) from dual;
16.0
</code></pre>
</li>
<li><p>幂运算函数: power(double a, double p)<br>返回a的p次幂,与pow功能相同</p>
<pre><code>hive&gt; select power(2,4) from dual;
16.0
</code></pre>
</li>
<li><p>开平方函数: sqrt(double a)<br>返回a的平方根</p>
<pre><code>hive&gt; select sqrt(16) from dual;
4.0
</code></pre>
</li>
<li><p>二进制函数: bin(BIGINT a)<br>返回a的二进制代码表示</p>
<pre><code>hive&gt; select bin(7) from dual;
111
</code></pre>
</li>
<li><p>十六进制函数: hex(BIGINT a)<br>如果变量是int类型，那么返回a的十六进制表示；如果变量是string类型，则返回该字符串的十六进制表示</p>
<pre><code>hive&gt; select hex(17) from dual;
11
hive&gt; select hex(‘abc’) from dual;
616263
</code></pre>
</li>
<li><p>反转十六进制函数: unhex(string a)<br>返回该十六进制字符串所代表的字符串</p>
<pre><code>hive&gt; select unhex(‘616263’) from dual;
abc
hive&gt; select unhex(‘11’) from dual;
-
hive&gt; select unhex(616263) from dual;
abc
</code></pre>
</li>
<li><p>进制转换函数: conv(BIGINT num, int from_base, int to_base)<br>将数值num从from_base进制转化到to_base进制</p>
<pre><code>hive&gt; select conv(17,10,16) from dual;
11
hive&gt; select conv(17,10,2) from dual;
10001
</code></pre>
</li>
<li><p>绝对值函数: abs(double a) || abs(int a)<br>返回数值a的绝对值</p>
<pre><code>hive&gt; select abs(-3.9) from dual;
3.9
hive&gt; select abs(10.9) from dual;
10.9
</code></pre>
</li>
<li><p>正取余函数: pmod(int a, int b) || pmod(double a, double b)<br>返回正的a除以b的余数</p>
<pre><code>hive&gt; select pmod(9,4) from dual;
1
hive&gt; select pmod(-9,4) from dual;
3
// 如果参数是一正一负，先都看成正数 4 * 3 = 12 &gt; 9，则余数是 12 - 9 = 3
// 找到比负数大的数，减去负数的绝对值
</code></pre>
</li>
<li><p>正弦函数: sin(double a)<br>返回a的正弦值</p>
<pre><code>hive&gt; select sin(0.8) from dual;
0.7173560908995228
</code></pre>
</li>
<li><p>反正弦函数: asin(double a)<br>返回a的反正弦值</p>
<pre><code>hive&gt; select asin(0.7173560908995228) from dual;
0.8
</code></pre>
</li>
<li><p>余弦函数: cos(double a)<br>返回a的余弦值</p>
<pre><code>hive&gt; select cos(0.9) from dual;
0.6216099682706644
</code></pre>
</li>
<li><p>反余弦函数: acos(double a)<br>返回a的反余弦值</p>
<pre><code>hive&gt; select acos(0.6216099682706644) from dual;
0.9
</code></pre>
</li>
<li><p>positive函数: positive(int a) || positive(double a)<br>返回a</p>
<pre><code>hive&gt; select positive(-10) from dual;
-10
hive&gt; select positive(12) from dual;
12
</code></pre>
</li>
<li><p>negative函数: negative(int a) || negative(double a)<br>返回-a</p>
<pre><code>hive&gt; select negative(-5) from dual;
5
hive&gt; select negative(8) from dual;
-8
</code></pre>
</li>
</ol>
<h3 id="七、集合操作函数"><a href="#七、集合操作函数" class="headerlink" title="七、集合操作函数"></a>七、集合操作函数</h3><ol>
<li><p>map类型大小：size(Map&lt;K.V&gt;)<br> 返回map类型的长度</p>
<pre><code> hive&gt; select size(map(&#39;100&#39;,&#39;tom&#39;,&#39;101&#39;,&#39;mary&#39;)) from lxw_dual;
 2
</code></pre>
</li>
<li><p>array类型大小：size(Array<T>)<br> 返回array类型的长度</p>
<pre><code> hive&gt; select size(array(&#39;100&#39;,&#39;101&#39;,&#39;102&#39;,&#39;103&#39;)) from lxw_dual;
 4
</code></pre>
</li>
<li><p>判断元素数组是否包含元素：array_contains(Array, value)<br> 返回 Array中是否包含元素 value</p>
<pre><code> hive&gt; select array_contains(array(1,2,3,4,5),3) from lxw1234; 
 true
</code></pre>
</li>
<li><p>获取map中所有value集合<br> map_values(Map)<br> 返回 Map中所有 value 的集合</p>
<pre><code> hive&gt; select map_values(map(&#39;k1&#39;,&#39;v1&#39;,&#39;k2&#39;,&#39;v2&#39;)) from lxw1234; 
 [&quot;v2&quot;,&quot;v1&quot;]
</code></pre>
</li>
<li><p>获取map中所有key集合<br> map_keys(Map)<br> 返回 Map中所有 key 的集合</p>
<pre><code> hive&gt; select map_keys(map(&#39;k1&#39;,&#39;v1&#39;,&#39;k2&#39;,&#39;v2&#39;)) from lxw1234; 
 [&quot;k2&quot;,&quot;k1&quot;]
</code></pre>
</li>
<li><p>数组排序<br> sort_array(Array)<br> 对 Array进行升序排序</p>
<pre><code> hive&gt; select sort_array(array(5,7,3,6,9)) from lxw1234;
 [3,5,6,7,9]
</code></pre>
</li>
</ol>
<h3 id="八、类型转换函数"><a href="#八、类型转换函数" class="headerlink" title="八、类型转换函数"></a>八、类型转换函数</h3><ol>
<li><p>二进制转换：binary(string|binary)<br> 将输入的值转换成二进制</p>
<pre><code> hive&gt; select binary(&#39;4&#39;);
 4
 hive&gt; select binary(&#39;1111&#39;);
 1111
 hive&gt; select binary(&#39;abd&#39;);
 abd
</code></pre>
</li>
<li><p>基础类型之间强制转换：cast(expr as <type>)<br> 返回值：Expected “&#x3D;” to follow “type”<br> 返回from之后的长度，如果是一张表就输出行数个expr</p>
<pre><code> hive&gt; select cast(1 as float);
 1.0
 hive&gt; select cast(2 as bigint) from tablss;
 2
 2
 2
 2
 2
 2
</code></pre>
</li>
</ol>
<h3 id="九、日期函数"><a href="#九、日期函数" class="headerlink" title="九、日期函数"></a>九、日期函数</h3><ol>
<li><p>UNIX时间戳转日期函数: from_unixtime(bigint unixtime[, string format])<br> 转化UNIX时间戳（从1970-01-01 00:00:00 UTC到指定时间的秒数）到当前时区的时间格式</p>
<pre><code> hive&gt; select from_unixtime(1323308943,&#39;yyyyMMdd&#39;) from dual;
 20111208
</code></pre>
</li>
<li><p>获取当前UNIX时间戳函数: unix_timestamp()<br> 获得当前时区的UNIX时间戳</p>
<pre><code> hive&gt; select unix_timestamp() from dual;
 1323309615
</code></pre>
</li>
<li><p>日期转UNIX时间戳函数: unix_timestamp(string date)<br> 转换格式为”yyyy-MM-dd HH:mm:ss”的日期到UNIX时间戳。如果转化失败，则返回0</p>
<pre><code> hive&gt; select unix_timestamp(&#39;2011-12-07 13:01:03&#39;) from dual;
 1323234063
</code></pre>
</li>
<li><p>指定格式日期转UNIX时间戳函数: unix_timestamp(string date, string pattern)<br> 转换pattern格式的日期到UNIX时间戳。如果转化失败，则返回0</p>
<pre><code> hive&gt; select unix_timestamp(&#39;20111207 13:01:03&#39;,&#39;yyyyMMdd HH:mm:ss&#39;) from dual;
 1323234063
</code></pre>
</li>
<li><p>日期时间转日期函数: to_date(string timestamp)<br> 返回日期时间字段中的日期部分</p>
<pre><code> hive&gt; select to_date(&#39;2011-12-08 10:03:01&#39;) from dual;
 2011-12-08
</code></pre>
</li>
<li><p>日期转年函数: year(string date)<br> 返回日期中的年</p>
<pre><code> hive&gt; select year(&#39;2011-12-08 10:03:01&#39;) from dual;
 2011
 hive&gt; select year(&#39;2012-12-08&#39;) from dual;
 2012
</code></pre>
</li>
<li><p>日期转月函数: month (string date)<br> 返回日期中的月份</p>
<pre><code> hive&gt; select month(&#39;2011-12-08 10:03:01&#39;) from dual;
 12
 hive&gt; select month(&#39;2011-08-08&#39;) from dual;
 8
</code></pre>
</li>
<li><p>日期转天函数: day (string date)<br> 返回日期中的天</p>
<pre><code> hive&gt; select day(&#39;2011-12-08 10:03:01&#39;) from dual;
 8
 hive&gt; select day(&#39;2011-12-24&#39;) from dual;
 24
</code></pre>
</li>
<li><p>日期转小时函数: hour (string date)<br> 返回日期中的小时</p>
<pre><code> hive&gt; select hour(&#39;2011-12-08 10:03:01&#39;) from dual;
 10
</code></pre>
</li>
<li><p>日期转分钟函数: minute (string date)<br>返回日期中的分钟</p>
<pre><code>hive&gt; select minute(&#39;2011-12-08 10:03:01&#39;) from dual;
3
</code></pre>
</li>
<li><p>日期转秒函数: second (string date)<br>返回日期中的秒</p>
<pre><code>hive&gt; select second(&#39;2011-12-08 10:03:01&#39;) from dual;
1
</code></pre>
</li>
<li><p>日期转周函数: weekofyear (string date)<br>返回日期在当前的周数</p>
<pre><code>hive&gt; select weekofyear(&#39;2011-12-08 10:03:01&#39;) from dual;
49
</code></pre>
</li>
<li><p>日期比较函数: datediff(string enddate, string startdate)<br>返回结束日期减去开始日期的天数</p>
<pre><code>hive&gt; select datediff(&#39;2012-12-08&#39;,&#39;2012-05-09&#39;) from dual;
213
</code></pre>
</li>
<li><p>日期增加函数: date_add(string startdate, int days)<br>返回开始日期startdate增加days天后的日期</p>
<pre><code>hive&gt; select date_add(&#39;2012-12-08&#39;,10) from dual;
2012-12-18
</code></pre>
</li>
<li><p>日期减少函数: date_sub (string startdate, int days)<br>返回开始日期startdate减少days天后的日期</p>
<pre><code>hive&gt; select date_sub(&#39;2012-12-08&#39;,10) from dual;
2012-11-28
</code></pre>
</li>
</ol>
<h3 id="十、条件函数"><a href="#十、条件函数" class="headerlink" title="十、条件函数"></a>十、条件函数</h3><ol>
<li><p>If函数: if(boolean testCondition, T valueTrue, T valueFalseOrNull)<br>  当条件testCondition为TRUE时，返回valueTrue；否则返回valueFalseOrNull</p>
<pre><code> hive&gt; select if(1=2,100,200) from dual;
 200
 hive&gt; select if(1=1,100,200) from dual;
 100
</code></pre>
</li>
<li><p>非空查找函数: COALESCE(T v1, T v2, …)<br> 返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL</p>
<pre><code> hive&gt; select COALESCE(null,&#39;100&#39;,&#39;50′) from dual;
 100
</code></pre>
</li>
<li><p>条件判断函数：CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END<br> 如果 a 等于 b ，那么返回 c ；如果 a 等于 d ，那么返回 e ；否则返回 f</p>
<pre><code> hive&gt; Select case 100 when 50 then &#39;tom&#39;   
 when 100 then &#39;mary&#39;   
 else &#39;tim&#39; end from dual;  
 mary
</code></pre>
</li>
</ol>
<h3 id="十一、字符串函数"><a href="#十一、字符串函数" class="headerlink" title="十一、字符串函数"></a>十一、字符串函数</h3><ol>
<li><p>字符ascii码函数：ascii(str)<br> 返回字符串str第一个字符的ascii码</p>
<pre><code> hive&gt; select ascii(‘abcde’) from dual;  
 97
</code></pre>
</li>
<li><p>base64字符串：base64(binary bin)<br> 将二进制bin转换成64位的字符串</p>
<pre><code> hive&gt; select base64(binary(‘lxw1234’));
 bHh3MTIzNA==
</code></pre>
</li>
<li><p>字符串连接函数：concat(string A, string B…)<br> 返回输入字符串连接后的结果，支持任意个输入字符串</p>
<pre><code> hive&gt; select concat(‘abc’,&#39;def’,&#39;gh’) from dual;  
 abcdefgh
</code></pre>
</li>
<li><p>带分隔符字符串连接函数：concat_ws(string SEP, string A, string B…)<br> 返回输入字符串连接后的结果，SEP表示各个字符串间的分隔符</p>
<pre><code> hive&gt; select concat_ws(‘,’,&#39;abc’,&#39;def’,&#39;gh’) from dual;  
 abc,def,gh
 hive&gt; select concat_ws(&#39;.&#39;,&#39;www&#39;,&#39;iteblog&#39;,&#39;com&#39;) from iteblog;
 www.iteblog.com
</code></pre>
</li>
<li><p>数组转换成字符串的函数：concat_ws(string SEP, array<string>)<br> 返回将数组链接成字符串后的结果，SEP 表示各个字符串间的分隔符</p>
<pre><code> hive&gt; select concat_ws(&#39;|&#39;,array(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;));
 a|b|c
</code></pre>
</li>
<li><p>小数位格式化成字符串函数：format_number(number x, int d)<br> 将数值X转换成”#,###,###.##”格式字符串，并保留d位小数，如果d为0，将进行四舍五入且不保留小数</p>
<pre><code> hive&gt; select format_number(5.23456,3) from lxw1234;
 5.235
</code></pre>
</li>
<li><p>字符串截取函数：substr(string A, int start) || substring(string A, int start)<br> 返回字符串A从start位置到结尾的字符串 </p>
<pre><code> hive&gt; select substr(‘abcde’,3) from dual;  
 cde  
 hive&gt; select substring(‘abcde’,3) from dual;  
 cde  
 hive&gt;  select substr(‘abcde’,-1) from dual;  （和ORACLE相同）  
 e
</code></pre>
</li>
<li><p>字符串截取函数：substr(string A, int start, int len) || substring(string A, int start, int len)<br> 返回字符串A从start位置开始，长度为len的字符串</p>
<pre><code> hiveselect substr(‘abcde’,3,2) from dual;  
 cd  
 hiveselect substring(‘abcde’,3,2) from dual;  
 cd  
 hive&gt;select substring(‘abcde’,-2,2) from dual;  
 de
</code></pre>
</li>
<li><p>字符串查找函数：instr(string str, string substr)<br> 返回值为int<br> 查找字符串str中子字符串substr出现的位置，如果查找失败将返回0，如果任一参数为Null将返回null，注意位置为从1开始的</p>
<pre><code> hive&gt; select instr(&#39;abcdf&#39;,&#39;df&#39;) from lxw1234;
 4
</code></pre>
</li>
<li><p>字符串长度函数：length(string A)<br>返回字符串A的长度</p>
<pre><code>hive&gt; select length(&#39;abcedfg&#39;) from dual;
7
</code></pre>
</li>
<li><p>字符串查找函数：locate(string substr, string str[, int pos])<br>查找字符串str中的pos位置后字符串substr第一次出现的位置</p>
<pre><code>hive&gt; select locate(&#39;a&#39;,&#39;abcda&#39;,1) from lxw1234;
1
hive&gt; select locate(&#39;a&#39;,&#39;abcda&#39;,2) from lxw1234;
5
</code></pre>
</li>
<li><p>字符串格式化函数：printf(String format, Obj… args)<br>按照printf风格格式输出字符串</p>
<pre><code>hive&gt; select printf(&quot;%08X&quot;,123) from lxw1234; 
0000007B
</code></pre>
</li>
<li><p>字符串转换成map函数：str_to_map(text[, delimiter1, delimiter2])<br>返回值：map&lt;string,string&gt;<br>将字符串str按照指定分隔符转换成Map，第一个参数是需要转换字符串，第二个参数是键值对之间的分隔符，默认为逗号;第三个参数是键值之间的分隔符，默认为”&#x3D;”</p>
<pre><code>hive&gt; select str_to_map(&#39;k1:v1,k2:v2&#39;) from lxw1234;
&#123;&quot;k2&quot;:&quot;v2&quot;,&quot;k1&quot;:&quot;v1&quot;&#125;
hive&gt; select str_to_map(&#39;k1=v1,k2=v2&#39;,&#39;,&#39;,&#39;=&#39;) from lxw1234;
&#123;&quot;k2&quot;:&quot;v2&quot;,&quot;k1&quot;:&quot;v1&quot;&#125;
</code></pre>
</li>
<li><p>base64解码函数：unbase64(string str)<br>将64位的字符串转换二进制值</p>
<pre><code>hive&gt; select unbase64(&#39;bHh3MTIzNA==&#39;) from lxw1234;
lxw1234
</code></pre>
</li>
<li><p>字符串转大写函数：upper(string A) || ucase(string A)<br>返回字符串A的大写格式</p>
<pre><code>hive&gt; select upper(‘abSEd’) from dual;  
ABSED  
hive&gt; select ucase(‘abSEd’) from dual;  
ABSED
</code></pre>
</li>
<li><p>字符串转小写函数：lower(string A) || lcase(string A)<br>返回字符串A的小写格式</p>
<pre><code>hive&gt; select lower(‘abSEd’) from dual;  
absed  
hive&gt; select lcase(‘abSEd’) from dual;  
absed
</code></pre>
</li>
<li><p>去空格函数：trim(string A)<br>去除字符串两边的空格</p>
<pre><code>hive&gt; select trim(‘ abc ‘) from dual;  
abc
</code></pre>
</li>
<li><p>左边去空格函数：ltrim(string A)<br>去除字符串左边的空格</p>
<pre><code>hive&gt; select ltrim(‘ abc ‘) from dual;  
abc
</code></pre>
</li>
<li><p>右边去空格函数：rtrim(string A)<br>去除字符串右边的空格</p>
<pre><code>hive&gt; select rtrim(‘ abc ‘) from dual;  
abc
</code></pre>
</li>
<li><p>正则表达式替换函数：regexp_replace(string A, string B, string C)<br>将字符串A中的符合java正则表达式B的部分替换为C。注意，在有些情况下要使用转义字符,类似oracle中的regexp_replace函数</p>
<pre><code>hive&gt; select regexp_replace(&#39;foobar&#39;, &#39;oo|ar&#39;, &#39;&#39;) from lxw_dual;
fb
</code></pre>
</li>
<li><p>正则表达式解析函数：regexp_extract(string subject, string pattern, int index)<br>将字符串subject按照pattern正则表达式的规则拆分，返回index指定的字符,其中的index是按照正则字符串（）的位置。注意，在有些情况下要使用转义字符</p>
<pre><code>hive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 1) from dual;  
the  
hive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 2) from dual;  
bar  
hive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 0) from dual;  
foothebar？？？？？
</code></pre>
</li>
<li><p>URL解析函数：parse_url(url, partToExtract[, key])？？？？？<br>解析URL字符串，partToExtract的选项包含[HOST,PATH,QUERY,REF,PROTOCOL,FILE,AUTHORITY,USERINFO]。</p>
<pre><code>parse_url(&#39;http://facebook.com/path/p1.php?query=1&#39;, &#39;HOST&#39;)返回&#39;facebook.com&#39;   
parse_url(&#39;http://facebook.com/path/p1.php?query=1&#39;, &#39;PATH&#39;)返回&#39;/path/p1.php&#39;   
parse_url(&#39;http://facebook.com/path/p1.php?query=1&#39;, &#39;QUERY&#39;)返回&#39;query=1&#39;，  
可以指定key来返回特定参数，例如  
parse_url(&#39;http://facebook.com/path/p1.php?query=1&#39;, &#39;QUERY&#39;,&#39;query&#39;)返回&#39;1&#39;，  
  
parse_url(&#39;http://facebook.com/path/p1.php?query=1#Ref&#39;, &#39;REF&#39;)返回&#39;Ref&#39;   
parse_url(&#39;http://facebook.com/path/p1.php?query=1#Ref&#39;, &#39;PROTOCOL&#39;)返回&#39;http&#39; 
</code></pre>
</li>
<li><p>json解析函数：get_json_object(string json_string, string path)<br>解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回NULL</p>
<pre><code>hive&gt; select  get_json_object(‘&#123;“store”:  
&gt;   &#123;“fruit”:\[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;,&#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],  
&gt;“bicycle”:&#123;“price”:19.95,”color”:”red”&#125;  
&gt;   &#125;,  
&gt;  “email”:”amy@only_for_json_udf_test.net”,  
&gt;  “owner”:”amy”  
&gt; &#125;  
&gt; ‘,’$.owner’) from dual;  
amy

select get_json_object(&#39;&#123;&quot;store&quot;:&#123;&quot;fruit&quot;:\[&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;]&#125;,&quot;owner&quot;:&quot;amy&quot;&#125;&#39;,&#39;$.store.fruit[0]&#39;) from test_msg limit 1;
</code></pre>
</li>
<li><p>空格字符串函数：space(int n)<br>返回长度为n的字符串</p>
<pre><code>hive&gt; select space(10) from dual;  
hive&gt; select length(space(10)) from dual;  
10
</code></pre>
</li>
<li><p>重复字符串函数：repeat(string str, int n)<br>返回重复n次后的str字符串</p>
<pre><code>hive&gt; select repeat(‘abc’,5) from dual;  
abcabcabcabcabc
</code></pre>
</li>
<li><p>左补足函数：lpad(string str, int len, string pad)<br>将str进行用pad进行左补足到len位</p>
<pre><code>hive&gt; select lpad(‘abc’,10,’td’) from dual;  
tdtdtdtabc
</code></pre>
</li>
<li><p>右补足函数：rpad(string str, int len, string pad)<br>将str进行用pad进行右补足到len位</p>
<pre><code>hive&gt; select rpad(‘abc’,10,’td’) from dual;  
abctdtdtdt
</code></pre>
</li>
<li><p>分割字符串函数: split(string str, string pat)<br>按照pat字符串分割str，会返回分割后的字符串数组</p>
<pre><code>hive&gt; select split(‘abtcdtef’,&#39;t’) from dual;  
[&quot;ab&quot;,&quot;cd&quot;,&quot;ef&quot;]
</code></pre>
</li>
<li><p>集合查找函数: find_in_set(string str, string strList)<br>返回str在strlist第一次出现的位置，strlist是用逗号分割的字符串。如果没有找该str字符，则返回0</p>
<pre><code>hive&gt; select find_in_set(‘ab’,&#39;ef,ab,de’) from dual;  
2  
hive&gt; select find_in_set(‘at’,&#39;ef,ab,de’) from dual;  
0
</code></pre>
</li>
<li><p>分词函数：sentences(string str, string lang, string locale)<br>返回值: array<array><br>返回输入 str 分词后的单词数组<br><strong>分词函数先分句子，句号.会看成是一个单词，逗号,会被拆开，叹号！会被拆成两个句子</strong></p>
<pre><code>hive&gt; select sentences(&#39;hello word!hello hive,hi hive,hello hive&#39;)
[[&quot;hello&quot;,&quot;word&quot;],[&quot;hello&quot;,&quot;hive&quot;,&quot;hi&quot;,&quot;hive&quot;,&quot;hello&quot;,&quot;hive&quot;]]  

select sentences(&#39;hello word,hello hive,hi hive,hello hive&#39;);
[[&quot;hello&quot;,&quot;word&quot;,&quot;hello&quot;,&quot;hive&quot;,&quot;hi&quot;,&quot;hive&quot;,&quot;hello&quot;,&quot;hive&quot;]]  

hive&gt;select sentences(&#39;hello word.hello hive,hi hive,hello hive&#39;);
[[&quot;hello&quot;,&quot;word.hello&quot;,&quot;hive&quot;,&quot;hi&quot;,&quot;hive&quot;,&quot;hello&quot;,&quot;hive&quot;]]
</code></pre>
</li>
<li><p>分词后统计一起出现频次最高的TOP-K<br>ngrams(array<array>, int N, int K, int pf)<br>返回值: array&lt;struct&lt;string,double&gt;&gt;<br>与 sentences()函数一起使用，分词后，统计分词结果中一起出现频次最高的 TOP-K 结果  </p>
<pre><code>hive&gt; select ngrams(sentences(&#39;hello word!hello hive,hi hive,hello hive&#39;),2,2);  
[&#123;&quot;ngram&quot;:[&quot;hello&quot;,&quot;hive&quot;],&quot;estfrequency&quot;:2.0&#125;,&#123;&quot;ngram&quot;:[&quot;hive&quot;,&quot;hi&quot;],&quot;estfrequency&quot;:1.0&#125;]
//该查询中，统计的是两个词在一起出现频次最高的 TOP\-2  
//结果中，hello 与 hive 同时出现 2 次
</code></pre>
</li>
<li><p>分词后统计与指定单词一起出现频次最高的TOP-K<br>context_ngrams(array<array>, array, int K, int pf)<br>返回值：array&lt;struct&lt;string,double&gt;&gt;<br>与 sentences()函数一起使用，分词后，统计分词结果中与数组中指定的单词一起出现(包括顺序)频次最高的 TOP-K 结果<br><strong>是紧跟着指定单词出现，可以跨越句子</strong></p>
<pre><code>hive&gt; select context_ngrams(sentences(&#39;hello word!hello hive,hi hive,hello hive&#39;),array(&#39;hello&#39;,null),3);
[&#123;&quot;ngram&quot;:[&quot;hive&quot;],&quot;estfrequency&quot;:2.0&#125;,&#123;&quot;ngram&quot;:[&quot;word&quot;],&quot;estfrequency&quot;:1.0&#125;]
//该查询中，统计的是与&#39;hello&#39;一起出现，并且在 hello 后面的频次最高的 TOP-3
//结果中，hello 与 hive 同时出现 2 次，hello 与 word 同时出现 1 次

hive&gt; select context_ngrams(sentences(&#39;hello word!hello hive,hi hive,hello hive&#39;),array(null,&#39;hive&#39;),3);
[&#123;&quot;ngram&quot;:[&quot;hello&quot;],&quot;estfrequency&quot;:2.0&#125;,&#123;&quot;ngram&quot;:[&quot;hi&quot;],&quot;estfrequency&quot;:1.0&#125;]
//该查询中，统计的是与&#39;hive&#39;一起出现，并且在 hive 之前的频次最高的 TOP-3

hive&gt;select context\_ngrams(sentences(&#39;hello word hello hive,hi hive,hello hive&#39;),array(null,&#39;hive&#39;),3);  
[&#123;&quot;ngram&quot;:[&quot;hello&quot;],&quot;estfrequency&quot;:2.0&#125;,&#123;&quot;ngram&quot;:[&quot;hi&quot;],&quot;estfrequency&quot;:1.0&#125;]

hive&gt;select context\_ngrams(sentences(&#39;hello hive,hello hive,hi hive,hello hive&#39;),array(null,&#39;hive&#39;),3);  
[&#123;&quot;ngram&quot;:[&quot;hello&quot;],&quot;estfrequency&quot;:3.0&#125;,&#123;&quot;ngram&quot;:[&quot;hi&quot;],&quot;estfrequency&quot;:1.0&#125;]

hive&gt;select context\_ngrams(sentences(&#39;hello hive,hello hive,hello hive,hi hive,hello hive&#39;),array(null,&#39;hive&#39;),3);
[&#123;&quot;ngram&quot;:[&quot;hello&quot;],&quot;estfrequency&quot;:4.0&#125;,&#123;&quot;ngram&quot;:[&quot;hi&quot;],&quot;estfrequency&quot;:1.0&#125;]
</code></pre>
</li>
<li><p>字符串反转函数： reverse(string A)<br>返回字符串A的反转结果</p>
<pre><code>hive&gt; select reverse(abcedfg’) from dual;
gfdecba  
</code></pre>
</li>
<li><p>重复字符串函数：repeat(string str, int n)<br>返回重复n次后的str字符串 </p>
<pre><code>hive&gt; select repeat(&#39;abc&#39;,5);
abcabcabcabcabc
</code></pre>
</li>
</ol>
<h3 id="十二、混合函数"><a href="#十二、混合函数" class="headerlink" title="十二、混合函数"></a>十二、混合函数</h3><ol>
<li><p>调用Java函数： java_method(class, method[, arg1[, arg2..]])<br> 返回值: varies<br> 调用 Java 中的方法处理数据  </p>
<pre><code> hive&gt; select reflect(&quot;java.net.URLEncoder&quot;, &quot;encode&quot;, &#39;http://lxw1234.com&#39;,&quot;UTF-8&quot;);
 http%3A%2F%2Flxw1234.com
 //该查询中调用 java.net.URLEncoder 中的encode方法，给该方法传的参数为&#39;http://lxw1234.com&#39;,&quot;UTF-8&quot;
</code></pre>
</li>
<li><p>调用Java函数：reflect(class, method[, arg1[, arg2..]])<br> 返回值: varies<br> 调用 Java 中的方法处理数据  </p>
<pre><code> hive&gt; select reflect(&quot;java.net.URLDecoder&quot;, &quot;decode&quot;,
 &#39;http%3A%2F%2Flxw1234.com&#39;,&quot;UTF-8&quot;);
 http://lxw1234.com
</code></pre>
</li>
<li><p>字符串的hash值：hash(a1[, a2…])<br> 返回值: int<br> 返回字符串的 hash 值  </p>
<pre><code> hive&gt; select hash(&#39;lxw1234.com&#39;);
 -809268416
</code></pre>
</li>
</ol>
<h3 id="十三、XPath解析XML函数"><a href="#十三、XPath解析XML函数" class="headerlink" title="十三、XPath解析XML函数"></a>十三、XPath解析XML函数</h3><ol>
<li><p>xpath<br> (string xmlstr,string xpath_expression)<br> 返回值: array<br> 从 xml 字符串中返回匹配到表达式的结果数组  </p>
<pre><code> //获取 xml 字符串中 a/b/节点的值
 hive&gt; select xpath(&#39;b1b2c1&#39;,&#39;a/b/text()&#39;) from lxw1234;
 [&quot;b1&quot;,&quot;b2&quot;]
 //获取 xml 字符串中所有名为 id 的属性值
 hive&gt; select xpath(&#39;b1b2&#39;,&#39;//@id&#39;) from lxw1234;
 [&quot;foo&quot;,&quot;bar&quot;]
</code></pre>
</li>
<li><p>xpath_string<br> xpath_string(string xmlstr,string xpath_expression)<br> 返回值: string<br> 默认情况下，从 xml 字符串中返回第一个匹配到表达式的节点的值  </p>
<pre><code> hive&gt; select xpath_string (&#39;b1b2&#39;, &#39;//b&#39;) from lxw1234;
 b1
 3.
 //指定返回匹配到哪一个节点
 hive&gt; select xpath_string (&#39;b1b2&#39;, &#39;//b[2]&#39;) from lxw1234;
 b2
</code></pre>
</li>
<li><p>xpath_boolean<br> xpath_boolean (string xmlstr,string xpath_expression)<br> 返回值: boolean<br> 返回 xml 字符串中是否匹配 xml 表达式  </p>
<pre><code> hive&gt; select xpath_boolean (&#39;b&#39;, &#39;a/b&#39;) from lxw1234;  
 true
 hive&gt; select xpath_boolean (&#39;10&#39;, &#39;a/b &lt; 10&#39;) from lxw1234;  
 false 
</code></pre>
</li>
<li><p>xpath_short, xpath_int, xpath_long<br> xpath_short (string xmlstr,string xpath_expression)<br> xpath_int (string xmlstr,string xpath_expression)<br> xpath_long (string xmlstr,string xpath_expression)<br> 返回值: int<br> 返回 xml 字符串中经过 xml 表达式计算后的值，如果不匹配，则返回0  </p>
<pre><code> hive&gt; SELECT xpath_int (&#39;this is not a number&#39;, &#39;a&#39;) FROM lxw1234;  
 0
 hive&gt; SELECT xpath_int (&#39;1248&#39;, &#39;sum(a/*)&#39;) FROM lxw1234;  
 15
 hive&gt; select xpath_long(&#39;10.511.2&#39;,&#39;sum(a/*)&#39;) from lxw1234;  
 21
</code></pre>
</li>
<li><p>xpath_float, xpath_double, xpath_number<br> xpath_float (string xmlstr,string xpath_expression)<br> xpath_double (string xmlstr,string xpath_expression)<br> path_number (string xmlstr,string xpath_expression)<br> 返回值: number<br> 返回 xml 字符串中经过 xml 表达式计算后的值，如果不匹配，则返回0  </p>
<pre><code> hive&gt; select xpath_double(&#39;10.511.2&#39;,&#39;sum(a/*)&#39;) from lxw1234;  
 21.7
</code></pre>
</li>
</ol>
<h3 id="十四、汇总统计函数（UDAF）"><a href="#十四、汇总统计函数（UDAF）" class="headerlink" title="十四、汇总统计函数（UDAF）"></a>十四、汇总统计函数（UDAF）</h3><ol>
<li><p>个数统计函数: count(*) || count(expr) || count(DISTINCT expr[, expr_.])  </p>
<p> count(*)统计检索出的行的个数，包括NULL值的行<br> count(expr)返回指定字段的非空值的个数<br> count(DISTINCTexpr[, expr_.])返回指定字段的不同的非空值的个数</p>
<pre><code> hive&gt; select count(*) from lxw_dual;
 20
 hive&gt; select count(distinct t) from lxw_dual;
 10
</code></pre>
</li>
<li><p>总和统计函数: sum(col) || sum(DISTINCT col)<br> sum(col)统计结果集中col的相加的结果<br> sum(DISTINCT col)统计结果中col不同值相加的结果</p>
<pre><code> hive&gt; select sum(t) from lxw_dual;
 100
 hive&gt; select sum(distinct t) from lxw_dual;
 70
</code></pre>
</li>
<li><p>平均值统计函数: avg(col) || avg(DISTINCT col)<br> avg(col)统计结果集中col的平均值<br> avg(DISTINCT col)统计结果中col不同值相加的平均值</p>
<pre><code> hive&gt; select avg(t) from lxw_dual;
 50
 hive&gt; select avg (distinct t) from lxw_dual;
 30
</code></pre>
</li>
<li><p>最小值统计函数: min(col)<br> 统计结果集中col字段的最小值</p>
<pre><code> hive&gt; select min(t) from lxw_dual;
 20
</code></pre>
</li>
<li><p>最大值统计函数: max(col)<br> 统计结果集中col字段的最大值</p>
<pre><code> hive&gt; select max(t) from lxw_dual;
 120
</code></pre>
</li>
<li><p>非空集合总体变量函数: var_pop(col)<br> 返回值: double<br> 统计结果集中col非空集合的总体变量（忽略null）</p>
</li>
<li><p>非空集合样本变量函数: var_samp (col)<br> 返回值: double<br> 统计结果集中col非空集合的样本变量（忽略null）</p>
</li>
<li><p>总体标准偏离函数: stddev_pop(col)<br> 返回值: double<br> 该函数计算总体标准偏离，并返回总体变量的平方根，其返回值与VAR_POP函数的平方根相同</p>
</li>
<li><p>样本标准偏离函数: stddev_samp (col)<br> 返回值: double<br> 该函数计算样本标准偏离</p>
</li>
<li><p>中位数函数: percentile  </p>
<ul>
<li><p>percentile(BIGINT col, p)<br>求准确的第pth个百分位数，p必须介于0和1之间，但是col字段目前只支持整数，不支持浮点数类型  </p>
<pre><code>  hive&gt;select percentile(id,0.2) from student;
  2.0
</code></pre>
</li>
<li><p>percentile(BIGINT col, array(p1 [, p2]…))<br>功能和上述类似，之后后面可以输入多个百分位数，返回类型也为array<double>，其中为对应的百分位数</p>
<pre><code>  hive&gt;select percentile(id,array(0.2,0.4)) from student；  
  [2.0,2.4000000000000004]
</code></pre>
</li>
</ul>
</li>
<li><p>近似中位数函数: percentile_approx(DOUBLE col, p [, B])<br>求近似的第pth个百分位数，p必须介于0和1之间，返回类型为double，但是col字段支持浮点类型。参数B控制内存消耗的近似精度，B越大，结果的准确度越高。默认为10,000。当col字段中的distinct值的个数小于B时，结果为准确的百分位数</p>
</li>
<li><p>直方图: histogram_numeric(col, b)<br>返回值：array&lt;struct {‘x’,’y’}&gt;<br>以b为基准计算col的直方图信息  </p>
<pre><code>hive&gt; select histogram_numeric(100,5) from lxw_dual;
[&#123;&quot;x&quot;:100.0,&quot;y&quot;:1.0&#125;]
</code></pre>
</li>
<li><p>集合去重数：collect_set(col)<br>只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段</p>
<pre><code>如下数据记录，要统计每种no下的score，这里就可以配合group by 产生奇效
no  score
1   2
1   3
1   3
2   2
2   4
2   4
hive&gt;select no，collect_set(score) from tablss group by no;
1    [2,3]
2    [2,4]

hive&gt; select * from student;
1    zhangsan    22
2    lisi    21
3    wangwu    17
4    zhaoliu    25
2    lisi    21
3    wangwu    17
4    zhaoliu    25
hive&gt;select name,collect_set(id) from student group by name;
lisi [2]
wangwu [3]
zhangsan [1]
zhaoliu [4]
</code></pre>
</li>
<li><p>集合不去重函数：collect_list (col)<br>返回值: array<br>将 col 字段合并成一个数组,不去重  </p>
<pre><code>hive&gt; select no,collect_list(score) from tablss group by no;
1    [2,3,3]
2    [2,4,4]

hive&gt; select name,collect_list(id) from student group by name;
lisi    [2,2]
wangwu    [3,3]
zhangsan    [1]
zhaoliu    [4,4]
</code></pre>
</li>
</ol>
<h3 id="十五、表格生成函数Table-Generating-Functions-UDTF"><a href="#十五、表格生成函数Table-Generating-Functions-UDTF" class="headerlink" title="十五、表格生成函数Table-Generating Functions (UDTF)"></a>十五、表格生成函数Table-Generating Functions (UDTF)</h3><ol>
<li><p>数组拆分成多行：explode  </p>
<ul>
<li><p>explode(array<TYPE> a)<br>  返回值：Array Type<br>  对于a中的每个元素，将生成一行且包含该元素  </p>
<pre><code>  hive&gt; select explode(split(concat_ws(&#39;,&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;,&#39;9&#39;),&#39;,&#39;)) from test.dual;  
  1  
  2  
  3  
  4  
  5  
  6  
  7  
  8  
  9
</code></pre>
</li>
</ul>
</li>
<li><p>Map拆分成多行：explode  </p>
<ul>
<li><p>explode(MAP)<br>  map中每个key-value对，生成一行，key为一列，value为一列</p>
<pre><code>  hive&gt; select explode(map(&#39;k1&#39;,&#39;v1&#39;,&#39;k2&#39;,&#39;v2&#39;));
  k1    v1
  k2    v2
</code></pre>
</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://mxxct4git.github.io/2018/11/16/Hive-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/panda-180.png">
      <meta itemprop="name" content="Mxxct">
      <meta itemprop="description" content="君子不器">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="猫熊小才天の书院">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/16/Hive-1/" class="post-title-link" itemprop="url">Hive</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-11-16 19:01:03" itemprop="dateCreated datePublished" datetime="2018-11-16T19:01:03+08:00">2018-11-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/Hive/" itemprop="url" rel="index"><span itemprop="name">Hive</span></a>
                </span>
            </span>

          
            <span id="/2018/11/16/Hive-1/" class="post-meta-item leancloud_visitors" data-flag-title="Hive" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2018/11/16/Hive-1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/11/16/Hive-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h3><p>要访问HDFS上的数据可以通过shell命令和API<br>如果要处理HDFS上的数据可以通过MapReduce，但MR的操作门槛比较高，eg：需要不断的进行调试，如果无法在本地运行，需要不断地导出jar包放到linux下进行调试  </p>
<p><strong>Hive</strong>是通过一种叫做HQL的类SQL语句来处理HDFS上的数据，但是和SQL不一样的是，HQL语言会转换为MR程序执行（select * from table不需要，因为select * from table只是对全部文件数据进行读取，并没有对数据进行操作）  </p>
<p><strong>Hive元数据</strong>：指在Hive中创建的数据库、表、视图等信息。这些元数据存储在关系型数据库中，比如MySQL  </p>
<h3 id="二、安装"><a href="#二、安装" class="headerlink" title="二、安装"></a>二、安装</h3><ol>
<li><p>解压<br> tar -zxvf apache-hive-1.2.1-bin.tar.gz  </p>
</li>
<li><p>设置环境变量<br> 需要用root权限，修改&#x2F;etc&#x2F;profile文件，退出root权限，输入source &#x2F;etc&#x2F;profile使环境变量生效  </p>
<pre><code> HIVE_HOME=/home/hadoop/app/apache-hive-1.0.0-bin  
 export PATH=$PATH:$HIVE_HOME/bin
</code></pre>
</li>
<li><p>修改hive-env.sh<br> 进入conf目录，将hive-env.sh.template复制并改名为hive-env.sh  </p>
<pre><code> #Set HADOOP_HOME to point to a specific hadoop install directory  
 HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0
</code></pre>
</li>
<li><p>修改hive-site.xml<br> 进入conf目录，将hive-site.xml放进去  </p>
<pre><code> &lt;configuration&gt;
   &lt;property&gt;
 &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
 &lt;value&gt;jdbc:mysql://localhost:3306/hive&lt;/value&gt;
 &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;
   &lt;/property&gt;
   
   &lt;property&gt;
 &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
 &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
 &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
   &lt;/property&gt;
   
   &lt;property&gt;
 &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
 &lt;value&gt;root&lt;/value&gt;
 &lt;description&gt;password to use against metastore database&lt;/description&gt;
   &lt;/property&gt;
   &lt;property&gt;
 &lt;name&gt;hive.hwi.listen.port&lt;/name&gt;
 &lt;value&gt;9999&lt;/value&gt;
 &lt;description&gt;This is the port the Hive Web Interface will listen on&lt;/description&gt;
   &lt;/property&gt;
   
   &lt;property&gt;
 &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt;
 &lt;value&gt;true&lt;/value&gt;
 &lt;description&gt;creates necessary schema on a startup if one doesn&#39;t exist. set this to false, after creating it once&lt;/description&gt;
   &lt;/property&gt;
   
   &lt;property&gt;
 &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
 &lt;value&gt;root&lt;/value&gt;
 &lt;description&gt;Username to use against metastore database&lt;/description&gt;
   &lt;/property&gt;
   
   &lt;property&gt;
 &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt;
 &lt;value&gt;/home/oracle/appdata/hivetmp/iotmp&lt;/value&gt;
 &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;
   &lt;/property&gt;
   
   &lt;property&gt;
 &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt;
 &lt;value&gt;/home/oracle/appdata/hivetmp/iotmp&lt;/value&gt;
 &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;
   &lt;/property&gt;
   
   &lt;property&gt;
 &lt;name&gt;hive.querylog.location&lt;/name&gt;
 &lt;value&gt;/home/oracle/appdata/hivetmp/iotmp&lt;/value&gt;
 &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;
   &lt;/property&gt;
 
 &lt;/configuration&gt;  
</code></pre>
</li>
<li><p>拷贝mysql-connector-java-5.0.8-bin.jar到hive 的lib下面  </p>
</li>
<li><p>在linux下安装mysql，需在root权限下进行操作 <strong>记得设置密码</strong>  </p>
<ul>
<li>安装mysql：yum install mysql-server mysql mysql-devel  </li>
<li>开启mysql服务：service mysqld start  </li>
<li>设置mysql密码：mysqladmin -u root password ‘root’  </li>
<li>mysql服务开机启动：chkconfig mysqld on</li>
</ul>
</li>
</ol>
<br/>
7. 把jline-2.12.jar拷贝到hadoop相应的目录下，替代jline-0.9.94.jar，否则启动会报错  

<pre><code>	cp hive/lib/jline-2.12.jar  hadoop-2.6.0/share/hadoop/yarn/lib/
</code></pre>
<h3 id="三、启动Hive"><a href="#三、启动Hive" class="headerlink" title="三、启动Hive"></a>三、启动Hive</h3><ol>
<li>MySql<br> 先启动MySQL创建hive数据库  create database hive;  </li>
<li>Hadoop<br> 启动hadoop sbin&#x2F;start-all.sh </li>
<li>Hive<br> 输入hive进入到hive shell</li>
</ol>
<p><strong>Hive默认存储在hadoop下的&#x2F;user&#x2F;hive&#x2F;warehouse下</strong>  </p>
<h3 id="四、Hive使用"><a href="#四、Hive使用" class="headerlink" title="四、Hive使用"></a>四、Hive使用</h3><ol>
<li><p>Hive默认数据库是default，直接创建table会建在defaul数据库中，同时会在&#x2F;user&#x2F;hive&#x2F;warehouse下创建对应表名的一个目录  </p>
</li>
<li><p>如果创建了一个新的数据库，会在&#x2F;user&#x2F;hive&#x2F;warehouse下新建一个对应库名的目录，以.db结尾，该新建数据库的所有table都会存在该目录下  </p>
</li>
<li><p>内部表的三种插入数据方式：  	</p>
<ul>
<li>insert  </li>
<li>hive的load命令，加载数据<br> <code>load data local inpath &#39;/home/hadoop/student.txt&#39; into table student;</code></li>
<li>直接把数据文件复制到相应的HDFS目录中</li>
</ul>
</li>
</ol>
<br/>
4. 外部表只有前两种插入数据方式  
	因为创建外部表的时候，不会在HDFS的数据库目录下创建该表的目录，在向表里添加数据时，也不在表目录中存储，删除外部表的时候，只会把表信息删除，不会删除表里的数据



<p>###五、使用Hive解析Json</p>
<ol>
<li><p>把json-serde-1.3.6-SNAPSHOT-jar-with-dependencies.jar复制到hive的lib目录下，重新进入hive  </p>
</li>
<li><p>建表：  </p>
<pre><code> create table user_movie(custid string, sno string, genreid string, movieid string) ROW FORMAT SERDE &#39;org.openx.data.jsonserde.JsonSerDe&#39;  STORED AS TEXTFILE;
</code></pre>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://mxxct4git.github.io/2018/11/16/Hadoop-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/panda-180.png">
      <meta itemprop="name" content="Mxxct">
      <meta itemprop="description" content="君子不器">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="猫熊小才天の书院">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/16/Hadoop-3/" class="post-title-link" itemprop="url">Yarn</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-11-16 18:59:29" itemprop="dateCreated datePublished" datetime="2018-11-16T18:59:29+08:00">2018-11-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/Yarn/" itemprop="url" rel="index"><span itemprop="name">Yarn</span></a>
                </span>
            </span>

          
            <span id="/2018/11/16/Hadoop-3/" class="post-meta-item leancloud_visitors" data-flag-title="Yarn" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2018/11/16/Hadoop-3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/11/16/Hadoop-3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ol>
<li><p>Yarn的简介<br>Yarn是Hadoop集群的资源管理系统,Hadoop2.0对MapReduce框架做了彻底的设计重构。<br>Hadoop1.x对MapReduce job的调度管理方式主要包括两部分功能：** ResourceManagement 资源管理** 和 <strong>JobScheduling&#x2F;JobMonitoring 任务调度监控</strong>。<br>到了Hadoop2.x也就是Yarn，它的目标是将这两部分功能分开，也就是分别用两个进程来管理这两个任务，将此拆分成了<strong>ResourceManager</strong> 和 <strong>ApplicationMaster</strong>。<br>需要注意的是，在Yarn中我们把job的概念换成了application，因为在新的Hadoop2.x中，运行的应用不只是MapReduce了，还有可能是其它应用如一个DAG（有向无环图Directed Acyclic Graph，例如storm应用）。  </p>
</li>
<li><p>Yarn的组件及架构<br> Yarn主要由以下几个组件组成：<br> 1. ResourceManager：Global（全局）的进程<br> 2. NodeManager：运行在每个节点上的进程<br> 3. ApplicationMaster：Application-specific（应用级别）的进程<br> - <em>Scheduler：是ResourceManager的一个组件</em><br> - <em>Container：节点上一组CPU和内存资源</em><br> Container是Yarn对计算机计算资源的抽象，它其实就是一组CPU和内存资源，所有的应用都会运行在Container中。<br> ApplicationMaster是对运行在Yarn中某个应用的抽象，它其实就是某个类型应用的实例，ApplicationMaster是应用级别的，它的主要功能就是向ResourceManager（全局的）申请计算资源（Containers）并且和NodeManager交互来执行和监控具体的task。<br> Scheduler是ResourceManager专门进行资源管理的一个组件，负责分配NodeManager上的Container资源，NodeManager也会不断发送自己Container使用情况给ResourceManager。  </p>
<p> ResourceManager和NodeManager两个进程主要负责系统管理方面的任务。<br> ResourceManager有一个Scheduler，负责各个集群中应用的资源分配。对于每种类型的每个应用，都会对应一个ApplicationMaster实例，ApplicationMaster通过和ResourceManager沟通获得Container资源来运行具体的job，并跟踪这个job的运行状态、监控运行进度。  </p>
</li>
<li><p>Yarn 框架相对于老的 MapReduce 框架什么优势呢？  </p>
<pre><code> 这个设计大大减小了 ResourceManager 的资源消耗，并且让监测每一个 Job 子任务 (tasks) 状态的程序分布式化了，更安全、更优美。
 在新的 Yarn 中，ApplicationMaster 是一个可变更的部分，用户可以对不同的编程模型写自己的 AppMst，让更多类型的编程模型能够跑在 Hadoop 集群中，可以参考 hadoop Yarn 官方配置模板中的 ``mapred-site.xml`` 配置。
 对于资源的表示以内存为单位 ( 在目前版本的 Yarn 中，没有考虑 cpu 的占用 )，比之前以剩余 slot 数目更合理。
 老的框架中，JobTracker 一个很大的负担就是监控 job 下的 tasks 的运行状况，现在，这个部分就扔给 ApplicationMaster 做了，而 ResourceManager 中有一个模块叫做 ApplicationsManager，它是监测 ApplicationMaster 的运行状况，如果出问题，会将其在其他机器上重启。
 Container 是 Yarn 为了将来作资源隔离而提出的一个框架。这一点应该借鉴了 Mesos 的工作，目前是一个框架，仅仅提供 java 虚拟机内存的隔离 ,hadoop 团队的设计思路应该后续能支持更多的资源调度和控制 , 既然资源表示成内存量，那就没有了之前的 map slot/reduce slot 分开造成集群资源闲置的尴尬情况。
</code></pre>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://mxxct4git.github.io/2018/11/16/Hadoop-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/panda-180.png">
      <meta itemprop="name" content="Mxxct">
      <meta itemprop="description" content="君子不器">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="猫熊小才天の书院">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/16/Hadoop-2/" class="post-title-link" itemprop="url">Hadoop生态圈</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-11-16 18:57:14" itemprop="dateCreated datePublished" datetime="2018-11-16T18:57:14+08:00">2018-11-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
            </span>

          
            <span id="/2018/11/16/Hadoop-2/" class="post-meta-item leancloud_visitors" data-flag-title="Hadoop生态圈" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2018/11/16/Hadoop-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/11/16/Hadoop-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一、Hadoop"><a href="#一、Hadoop" class="headerlink" title="一、Hadoop"></a>一、Hadoop</h2><ol>
<li><p>Hadoop是一个由Apache基金会所开发的分布式系统基础架构  </p>
</li>
<li><p>充分利用集群的威力进行高速运算和存储，具有可靠、高效、可伸缩的特点  </p>
</li>
<li><p>Hadoop的核心是Yarn、HDFS和MapReduce</p>
<ul>
<li>适合大数据的分布式存储与计算平台</li>
<li>HDFS: Hadoop Distributed File System分布式文件系统</li>
<li>MapReduce：并行计算框架</li>
</ul>
</li>
</ol>
<h2 id="二、HDFS-Hadoop分布式文件系统"><a href="#二、HDFS-Hadoop分布式文件系统" class="headerlink" title="二、HDFS(Hadoop分布式文件系统)"></a>二、HDFS(Hadoop分布式文件系统)</h2><ol>
<li>Hadoop Distributed File System  </li>
<li>HDFS是Hadoop体系中数据存储管理的基础  </li>
<li>高容错；高吞吐量；在项目里处理大数据集；流式访问文件系统数据；可以构建在普通的硬件之上  </li>
<li>一次写入多次读取；数据以块的形式，同时分布在集群不同物理机器上</li>
</ol>
<h2 id="三、MapReduce-分布式计算框架"><a href="#三、MapReduce-分布式计算框架" class="headerlink" title="三、MapReduce(分布式计算框架)"></a>三、MapReduce(分布式计算框架)</h2><ol>
<li>MapReduce是一种分布式计算模型，用以进行大数据量的计算</li>
<li>它屏蔽了分布式计算框架细节，将计算抽象成map和reduce两部分，其中Map对数据集上的独立元素进行指定的操作，生成键-值对形式中间结果。Reduce则对中间结果中相同“键”的所有“值”进行规约，以得到最终结果</li>
</ol>
<h2 id="四、Hbase-分布式列存数据库"><a href="#四、Hbase-分布式列存数据库" class="headerlink" title="四、Hbase(分布式列存数据库)"></a>四、Hbase(分布式列存数据库)</h2><ol>
<li><p>HBase是一个建立在HDFS之上，面向列的针对结构化数据的可伸缩、高可靠、高性能、分布式和面向列的动态模式数据库</p>
</li>
<li><p>HBase利用Hadoop HDFS作为其文件存储系统，利用Hadoop MapReduce来处理HBase中的海量数据</p>
</li>
<li><p>一次写多次读，对修改支持很少</p>
</li>
<li><p>表 行键 列族 列 值</p>
<ul>
<li>Google Bigtable的开源实现</li>
<li>列式数据库</li>
<li>可集群化</li>
<li>可以使用shell、web、api等多种方式访问</li>
<li>适合高读写(insert)的场景</li>
<li>HQL查询语言</li>
<li>NoSQL的典型代表产品</li>
</ul>
</li>
</ol>
<h2 id="五、Hive-数据仓库"><a href="#五、Hive-数据仓库" class="headerlink" title="五、Hive(数据仓库)"></a>五、Hive(数据仓库)</h2><ol>
<li><p>Hive定义了一种类似SQL的查询语言(HQL),将SQL转化为MapReduce任务在Hadoop上执行</p>
</li>
<li><p>通常用于离线分析</p>
<ul>
<li>数据仓库工具。可以把Hadoop下的原始结构化数据变成Hive中的表</li>
<li>支持一种与SQL几乎完全相同的语言HiveQL。除了不支持更新、索引和事务，几乎SQL的其它特征都能支持</li>
<li>可以看成是从SQL到Map-Reduce的映射器</li>
<li>提供shell、JDBC&#x2F;ODBC、Thrift、Web等接口</li>
</ul>
</li>
</ol>
<blockquote>
<p>数据仓库和关系型数据库的关系<br>一般来说，一个项目通常都有专门的数据库进行存储数据，这时候用到的一般都是关系型数据库，存储的数据都是较少的有针对性的数据，可以快速的存取<br>数据仓库中存储的数据是庞大的，并且种类繁多，所能执行的语句也很多  </p>
</blockquote>
<h2 id="六、Sqoop-数据ETL-同步工具"><a href="#六、Sqoop-数据ETL-同步工具" class="headerlink" title="六、Sqoop(数据ETL&#x2F;同步工具)"></a>六、Sqoop(数据ETL&#x2F;同步工具)</h2><ol>
<li><p>SQL-to-Hadoop</p>
</li>
<li><p>主要用于传统数据库和Hadoop之前传输数据,可以将一个关系型数据库中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中  </p>
</li>
<li><p>数据的导入和导出本质上是Mapreduce程序，充分利用了MR的并行化和容错性</p>
<ul>
<li>用于在Hadoop和关系型数据库之间交换数据</li>
<li>通过JDBC接口连入关系型数据库</li>
</ul>
</li>
</ol>
<h2 id="七、Flume-日志收集工具"><a href="#七、Flume-日志收集工具" class="headerlink" title="七、Flume(日志收集工具)"></a>七、Flume(日志收集工具)</h2><ol>
<li>Cloudera开源的日志收集系统，具有分布式、高可靠、高容错、易于定制和扩展的特点  </li>
<li>它将数据从产生、传输、处理并最终写入目标的路径的过程抽象为数据流，在具体的数据流中，数据源支持在Flume中定制数据发送方，从而支持收集各种不同协议数据  </li>
<li>同时，Flume数据流提供对日志数据进行简单处理的能力，如过滤、格式转换等。此外，Flume还具有能够将日志写往各种数据目标（可定制）的能力</li>
</ol>
<h2 id="八、Yarn-分布式资源管理器"><a href="#八、Yarn-分布式资源管理器" class="headerlink" title="八、Yarn(分布式资源管理器)"></a>八、Yarn(分布式资源管理器)</h2><ol>
<li>将JobTracker的两个主要功能（资源管理和作业调度&#x2F;监控）分离，主要方法是创建一个全局的ResourceManager（RM）和若干个针对应用程序的ApplicationMaster（AM）</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://mxxct4git.github.io/2018/11/16/Hadoop-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/panda-180.png">
      <meta itemprop="name" content="Mxxct">
      <meta itemprop="description" content="君子不器">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="猫熊小才天の书院">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/11/16/Hadoop-1/" class="post-title-link" itemprop="url">Hadoop1.0 和 Hadoop2.0</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-11-16 18:54:37" itemprop="dateCreated datePublished" datetime="2018-11-16T18:54:37+08:00">2018-11-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
            </span>

          
            <span id="/2018/11/16/Hadoop-1/" class="post-meta-item leancloud_visitors" data-flag-title="Hadoop1.0 和 Hadoop2.0" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2018/11/16/Hadoop-1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/11/16/Hadoop-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="1-从Hadoop整体框架来说"><a href="#1-从Hadoop整体框架来说" class="headerlink" title="1.从Hadoop整体框架来说"></a>1.从Hadoop整体框架来说</h3><p>1.1 Hadoop1.0即第一代Hadoop，由分布式存储系统HDFS和分布式计算框架MapReduce组成，其中HDFS由一个NameNode和多个DateNode组成，MapReduce由一个JobTracker和多个TaskTracker组成。<br>1.2 Hadoop2.0为克服Hadoop1.0中的不足：<strong>针对Hadoop1.0单NameNode制约HDFS的扩展性问题</strong>，提出HDFS Federation，它让多个NameNode分管不同的目录进而实现访问隔离和横向扩展，同时彻底解决了NameNode单点故障问题；<br>针对Hadoop1.0中的MapReduce在扩展性和多框架支持等方面的不足，它将JobTracker中的资源管理和作业控制分开，分别由ResourceManager（负责所有应用程序的资源分配）和ApplicationMaster（负责管理一个应用程序）实现，即<strong>引入了资源管理框架Yarn</strong>。同时Yarn作为Hadoop2.0中的资源管理系统，它是一个通用的资源管理模块，可为各类应用程序进行资源管理和调度，不仅限于MapReduce一种框架，也可以为其他框架使用，如Tez、Spark、Storm等</p>
<h3 id="2-从MapReduce计算框架来说"><a href="#2-从MapReduce计算框架来说" class="headerlink" title="2.从MapReduce计算框架来说"></a>2.从MapReduce计算框架来说</h3><p>2.1 MapReduce1.0计算框架主要由三部分组成：编程模型、数据处理引擎和运行时环境。它的基本编程模型是将问题抽象成Map和Reduce两个阶段，其中Map阶段将输入的数据解析成key&#x2F;value，迭代调用map()函数处理后，再以key&#x2F;value的形式输出到本地目录，Reduce阶段将key相同的value进行规约处理，并将最终结果写到HDFS上；它的数据处理引擎由MapTask和ReduceTask组成，分别负责Map阶段逻辑和Reduce阶段的逻辑处理；它的运行时环境由一个JobTracker和若干个TaskTracker两类服务组成，其中JobTracker负责资源管理和所有作业的控制，TaskTracker负责接收来自JobTracker的命令并执行它。<br>2.2 MapReducer2.0具有与MRv1相同的编程模型和数据处理引擎，唯一不同的是运行时环境。MRv2是在MRv1基础上经加工之后，运行于资源管理框架Yarn之上的计算框架MapReduce。它的运行时环境不再由JobTracker和TaskTracker等服务组成，而是变为通用资源管理系统Yarn和作业控制进程ApplicationMaster，其中Yarn负责资源管理的调度而ApplicationMaster负责作业的管理。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/23/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><span class="page-number current">24</span><a class="page-number" href="/page/25/">25</a><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" href="/page/25/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Mxxct"
      src="/images/panda-180.png">
  <p class="site-author-name" itemprop="name">Mxxct</p>
  <div class="site-description" itemprop="description">君子不器</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">127</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">69</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/mxxt" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;mxxt" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:386965035@qq.com" title="邮箱 → mailto:386965035@qq.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>邮箱</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mxxct</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>















  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : true,
      notify     : true,
      appId      : 'FeVPpNOBXhL1P240cNkmAKc3-gzGzoHsz',
      appKey     : 'TJ9vKn2xQ16geSxRr80seK0S',
      placeholder: "来说点什么吧~~~",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
